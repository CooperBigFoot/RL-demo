# Technical Description: The "Whispering River" Dam Reinforcement Learning Environment

## **Version 1.0**

#### **1. Introduction**

The "Whispering River" Dam is a simulated environment designed for reinforcement learning. The objective is to train an agent to make optimal daily water release decisions from a single reservoir. The agent must learn to balance three competing objectives: maximizing hydropower generation, ensuring flood safety, and maintaining a minimum environmental outflow. The environment is characterized by stochastic daily inflows and imperfect, noisy forecasts, forcing the agent to develop a robust operational policy under uncertainty.

This document specifies the formal components of this environment according to the Markov Decision Process (MDP) framework.

#### **2. Environment Components**

The environment is modeled as a finite-horizon, stochastic MDP. An episode consists of a full year (365 timesteps), where each timestep $t$ represents one day.

* **Agent:** The dam operator.
* **Environment:** The reservoir, river system, and inflow dynamics.

#### **3. State Space ($\mathcal{S}$)**

The state $s_t \in \mathcal{S}$ observed by the agent at the beginning of day $t$ is a vector containing the following information:

$s_t = [v_t^{pct}, \sin(\theta_t), \cos(\theta_t), \hat{I}_{t \to t+9}]$

Where:

* **$v_t^{pct} \in [0, 1]$**: The current reservoir volume expressed as a percentage of the maximum capacity. $v_t^{pct} = \frac{V_t}{V_{max}}$, where $V_t$ is the absolute volume.
* **$\sin(\theta_t), \cos(\theta_t)$**: A cyclical encoding of the time of year, where $\theta_t = \frac{2\pi \cdot (\text{day of year})}{365}$. This representation ensures the agent understands the seasonal proximity of the end and beginning of the year.
* **$\hat{I}_{t \to t+9}$**: A vector of 10 values representing the forecasted mean daily inflow for the next 10 days, $[\hat{I}_t, \hat{I}_{t+1}, ..., \hat{I}_{t+9}]$. These forecasts are generated by adding noise to the true future inflows, with the noise variance increasing linearly for more distant days.
    $\hat{I}_{t+k} = I_{true, t+k} + \epsilon_k$, where the variance of $\epsilon_k$ increases with $k$.

#### **4. Action Space ($\mathcal{A}$)**

The action space is continuous, defining the agent's release decision for day $t$.

* **$a_t \in [0, 1]$**: A scalar value representing the fraction of the *releasable water volume* to be discharged over the day.

The actual release volume, $R_t$, is calculated as:
$R_t = a_t \cdot (V_t - V_{min})$

However, the release is subject to an operational constraint:
$R_t^{eff} = \min(R_t, 0.10 \cdot V_t, V_t - V_{dead})$

Where $R_t^{eff}$ is the effective, or actual, release after constraints are applied.

#### **5. Transition Dynamics ($P(s_{t+1}|s_t, a_t)$)**

The environment transitions from state $s_t$ to $s_{t+1}$ based on the agent's action $a_t$ and the stochastic daily inflow.

1. The agent chooses action $a_t$.
2. The effective release volume $R_t^{eff}$ is calculated.
3. The true inflow for the day, $I_{true, t}$, is realized from a pre-defined hydrograph with stochastic noise.
4. The reservoir volume for the next state, $V_{t+1}$, is updated according to the water balance equation:
    $V_{t+1} = V_t - R_t^{eff} + I_{true, t}$
5. The new state $s_{t+1}$ is constructed using the updated volume $V_{t+1}$, the next day of the year, and the new 10-day forecast.

#### **6. Reward Function ($\mathcal{R}$)**

The reward $r_t$ is calculated at the end of day $t$ after action $a_t$ is taken and the new state $s_{t+1}$ is observed. It is a weighted sum of rewards and penalties corresponding to the three core objectives.

$r_t = w_h \cdot r_{hydro} + w_f \cdot p_{flood} + w_e \cdot p_{env}$

* **Hydropower Reward ($r_{hydro}$):** Proportional to the potential energy of the released water.
    $r_{hydro} = R_t^{eff} \cdot v_t^{pct}$
    This incentivizes releasing water when the reservoir level is high.

* **Flood Control Penalty ($p_{flood}$):** A penalty is applied if the resulting water volume exceeds a predefined safety threshold, $V_{safe}$ (e.g., 85% of $V_{max}$).
    $p_{flood} = \begin{cases} -C_{risk} \cdot (V_{t+1} - V_{safe}) & \text{if } V_{t+1} > V_{safe} \\ 0 & \text{otherwise} \end{cases}$
    where $C_{risk}$ is a penalty coefficient.

* **Environmental Flow Penalty ($p_{env}$):** A penalty is applied if the release does not meet the minimum required environmental flow, $R_{env,min}$.
    $p_{env} = \begin{cases} -C_{env} \cdot (R_{env,min} - R_t^{eff}) & \text{if } R_t^{eff} < R_{env,min} \\ 0 & \text{otherwise} \end{cases}$
    where $C_{env}$ is a penalty coefficient. A small, logarithmic bonus can be added for exceeding this minimum to reward beneficial releases without encouraging wasteful dumping.

The weights ($w_h, w_f, w_e$) are hyperparameters that balance the trade-offs between the objectives.

#### **7. Episode Termination**

An episode terminates under the following conditions:

* **End of Horizon:** The episode concludes after 365 days.
* **Catastrophic Flood:** If $V_{t+1} > V_{max}$. This state transition results in a large negative terminal reward.
* **Critical Drought:** If $V_{t+1} < V_{min}$. This state transition also results in a large negative terminal reward.

The agent's goal is to maximize the expected cumulative reward, $\mathbb{E}\left[\sum_{t=0}^{364} \gamma^t r_t\right]$, where $\gamma$ is the discount factor. By maximizing this sum, the agent learns to make decisions that yield high long-term rewards, navigating the complex trade-offs inherent in reservoir management.
